---
title: "KNN Package Introduction"
author: "Meiqi Zhu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{KNN Package Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(KNN)
```

# Introduction to KNN Package

The KNN package provides an efficient implementation of the K-Nearest Neighbors algorithm for classification using pure R code. This vignette demonstrates the package's functionality and compares it with the existing `class::knn` function.

## Installation

```{r install, eval=FALSE}
# Install from GitHub
devtools::install_github("ParsecLink/KNN")
```

## Basic Usage

### Using `knn_r()` function

```{r basic_usage}
library(KNN)
data(iris)

set.seed(123)
train_idx <- sample(1:nrow(iris), 100)
train <- iris[train_idx, 1:4]
test <- iris[-train_idx, 1:4]
train_labels <- iris[train_idx, 5]

# Make predictions
predictions <- knn_r(train, test, train_labels, k = 3)

# Check results
table("Predicted" = predictions, "Actual" = iris[-train_idx, 5])
```

### Using the formula interface with `knn_predict()`

```{r formula_interface}
set.seed(123)
train_idx <- sample(1:nrow(iris), 100)
train_data <- iris[train_idx, ]
test_data <- iris[-train_idx, ]

# Formula interface
predictions_formula <- knn_predict(Species ~ ., train_data, test_data, k = 3)

# Results
table("Predicted" = predictions_formula, "Actual" = test_data$Species)
```

## Distance Methods

The package supports both Euclidean and Manhattan distance metrics:

```{r distance_methods}
set.seed(123)
small_train_idx <- sample(1:nrow(iris), 30)
small_train <- iris[small_train_idx, 1:4]
small_test <- iris[131:140, 1:4]
small_labels <- iris[small_train_idx, 5]

# Euclidean distance (default)
pred_euclidean <- knn_r(small_train, small_test, small_labels, k = 3, distance_method = "euclidean")

# Manhattan distance
pred_manhattan <- knn_r(small_train, small_test, small_labels, k = 3, distance_method = "manhattan")

# Compare results
data.frame(
  Euclidean = pred_euclidean,
  Manhattan = pred_manhattan,
  Actual = iris[131:140, 5]
)
```

# Comparison with Existing Implementation

## Correctness Comparison

We'll compare our implementation with the `class::knn` function to ensure correctness.

```{r correctness}
library(class)

set.seed(123)
train_idx <- sample(1:nrow(iris), 100)
train <- iris[train_idx, 1:4]
test <- iris[-train_idx, 1:4]
train_labels <- iris[train_idx, 5]

# Our implementation
our_predictions <- knn_r(train, test, train_labels, k = 3)

# class::knn implementation
class_predictions <- knn(train, test, train_labels, k = 3)

# Check if they are identical
identical(as.character(our_predictions), as.character(class_predictions))

# Detailed comparison
comparison <- data.frame(
  Our_KNN = our_predictions,
  Class_KNN = class_predictions,
  Actual = iris[-train_idx, 5],
  Match = our_predictions == class_predictions
)

head(comparison, 10)

# Accuracy comparison
our_accuracy <- mean(our_predictions == iris[-train_idx, 5])
class_accuracy <- mean(class_predictions == iris[-train_idx, 5])

cat("Our KNN accuracy:", round(our_accuracy, 4), "\n")
cat("class::knn accuracy:", round(class_accuracy, 4), "\n")
```

## Performance Comparison

Let's compare the performance of both implementations using the `bench` package.

```{r performance}
library(bench)

set.seed(123)
# Use a smaller dataset for performance testing to avoid long run times
small_idx <- sample(1:nrow(iris), 80)
small_train <- iris[small_idx, 1:4]
small_test <- iris[setdiff(1:50, small_idx), 1:4]  # Use first 50 rows for test
small_labels <- iris[small_idx, 5]

# Benchmark both implementations
performance_results <- mark(
  Our_KNN = knn_r(small_train, small_test, small_labels, k = 3),
  Class_KNN = knn(small_train, small_test, small_labels, k = 3),
  iterations = 100,
  check = FALSE  # We already verified correctness above
)

print(performance_results)

# Plot the results
plot(performance_results)
```

## Memory Usage Comparison

```{r memory}
library(profmem)

set.seed(123)
small_idx <- sample(1:nrow(iris), 60)
small_train <- iris[small_idx, 1:4]
small_test <- iris[setdiff(1:40, small_idx), 1:4]
small_labels <- iris[small_idx, 5]

# Memory usage for our implementation
mem_our <- profmem({
  our_result <- knn_r(small_train, small_test, small_labels, k = 3)
})

# Memory usage for class::knn
mem_class <- profmem({
  class_result <- knn(small_train, small_test, small_labels, k = 3)
})

cat("Our KNN memory usage:\n")
print(total(mem_our))

cat("\nclass::knn memory usage:\n")
print(total(mem_class))
```

# Advanced Examples

## Working with Different Datasets

```{r other_datasets}
# Example with mtcars dataset
data(mtcars)

# Convert mpg to a classification problem (high vs low mileage)
mtcars$mpg_category <- ifelse(mtcars$mpg > median(mtcars$mpg), "high", "low")
mtcars$mpg_category <- factor(mtcars$mpg_category)

set.seed(123)
train_idx <- sample(1:nrow(mtcars), 25)
train_data <- mtcars[train_idx, ]
test_data <- mtcars[-train_idx, ]

# Make predictions
car_predictions <- knn_predict(mpg_category ~ wt + hp + disp, train_data, test_data, k = 3)

# Results
table("Predicted" = car_predictions, "Actual" = test_data$mpg_category)
```

## Handling Different Values of K

```{r different_k}
set.seed(123)
train_idx <- sample(1:nrow(iris), 100)
train <- iris[train_idx, 1:4]
test <- iris[-train_idx, 1:4]
train_labels <- iris[train_idx, 5]

# Test different k values
k_values <- c(1, 3, 5, 7, 9)
accuracies <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  predictions <- knn_r(train, test, train_labels, k = k_values[i])
  accuracies[i] <- mean(predictions == iris[-train_idx, 5])
}

results_df <- data.frame(k = k_values, Accuracy = accuracies)
print(results_df)

# Plot accuracy vs k
plot(results_df$k, results_df$Accuracy, type = "b", 
     xlab = "k Value", ylab = "Accuracy",
     main = "KNN Accuracy vs k Value")
```

# Conclusion

The KNN package provides a pure R implementation of the K-Nearest Neighbors algorithm that:

- Produces identical results to `class::knn`
- Offers both direct and formula-based interfaces
- Supports multiple distance metrics
- Is well-tested with comprehensive unit tests

This implementation serves as both a practical tool for classification tasks and an educational resource for understanding how KNN works internally.
